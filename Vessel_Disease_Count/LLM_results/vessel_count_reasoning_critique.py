# -*- coding: utf-8 -*-
"""
Diseased Vessel Counting — Reasoning Critique (Checker Model)
Corresponds to Mcheck in the SCOUT framework (Strategy 3: Reasoning Critique).
Uses Qwen3-32B via Aliyun DashScope API to audit the chain-of-thought trace from Mmain.
"""

import os
import json
import time
import csv
import re
import threading
from collections import deque
from concurrent.futures import ThreadPoolExecutor, as_completed

import pandas as pd
from openai import OpenAI

# ========================== Configuration ==========================

API_KEY = os.getenv("DASHSCOPE_API_KEY", "sk-XXX")
BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"
MODEL_NAME = "qwen3-32b"

INPUT_CSV_FILE = "disease_vessels_DeepSeek_Aliyun_MultiThread1.csv"
OUTPUT_CSV_FILE = "disease_vessels_Audit_Results.csv"

ID_COLUMN = "hadm_id"
REASONING_COLUMN = "reasoning_content"

MAX_WORKERS = 40
CALLS_PER_MINUTE = 200
WINDOW_SECONDS = 65

file_lock = threading.Lock()
client = OpenAI(api_key=API_KEY, base_url=BASE_URL)

# ========================== Audit Prompt ==========================

AUDIT_SYSTEM_PROMPT = """You are a strict Medical Logic Auditor.
Your task is to review a Reasoning Trace generated by an AI for coronary vessel disease counting.
Check whether the reasoning strictly follows the Rules below. You do NOT need the original medical record — only assess the internal logical consistency of the reasoning.

[Rules (Ground Truth)]
1. **Threshold criteria**:
   - LAD/LCX/RCA and their branches: stenosis must be ≥70%, or FFR/QFR positive / CTO / ISR to qualify as diseased.
   - Stenosis <70% (e.g., 40-50%, 60%) without functional evidence must NOT be counted.
2. **LM (left main) special rule**:
   - LM stenosis ≥50% must be treated as both LAD and LCX diseased (involved_vessels should include both).
3. **Deduplication rule**:
   - Multiple lesions in the same trunk (including branches) count as only +1.
   - Final count must equal the deduplicated number of involved trunks (LAD, LCX, RCA).

[Audit checklist]
1. **Data-conclusion consistency**: Do stenosis values cited in the reasoning match the conclusions drawn? (e.g., citing 60% but concluding 'diseased' is a logical error.)
2. **LM logic**: If LM ≥50% is confirmed, was two-vessel involvement (LAD + LCX) correctly deduced?
3. **Arithmetic**: Does the length of the involved_vessels list equal the final count?

[Output format]
If any logical error or rule violation is found, output "FAIL" with a brief explanation; if the reasoning is fully self-consistent, output "PASS".

JSON format:
{
  "status": "PASS" or "FAIL",
  "critique": "If FAIL, describe the specific logical contradiction. If PASS, leave empty."
}
"""


# ========================== Utilities ==========================

class ThreadSafeRateLimiter:
    """Sliding-window rate limiter (thread-safe)."""

    def __init__(self, max_calls: int, window_seconds: int):
        self.max_calls = max_calls
        self.window_seconds = window_seconds
        self.ts = deque()
        self.lock = threading.Lock()

    def wait(self):
        with self.lock:
            now = time.time()
            while self.ts and (now - self.ts[0]) >= self.window_seconds:
                self.ts.popleft()
            if len(self.ts) >= self.max_calls:
                sleep_s = self.window_seconds - (now - self.ts[0]) + 0.1
                if sleep_s > 0:
                    time.sleep(sleep_s)
                now = time.time()
                while self.ts and (now - self.ts[0]) >= self.window_seconds:
                    self.ts.popleft()
            self.ts.append(time.time())


def parse_json_from_content(content: str):
    """Extract JSON object from model response."""
    text = content.strip()
    if text.startswith("```"):
        text = re.sub(r"^```[a-zA-Z0-9]*\n", "", text).strip()
        if text.endswith("```"):
            text = text[:-3].strip()
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        start, end = text.find("{"), text.rfind("}")
        if start != -1 and end > start:
            try:
                return json.loads(text[start:end + 1])
            except Exception:
                pass
    return None


def call_audit_model(reasoning_content: str, limiter: ThreadSafeRateLimiter):
    """Call DashScope API to audit reasoning trace."""
    user_content = f"""[Input Reasoning Trace]
{reasoning_content}

Audit the above reasoning against the Rules and Audit Checklist, then output JSON."""

    messages = [
        {"role": "system", "content": AUDIT_SYSTEM_PROMPT},
        {"role": "user", "content": user_content},
    ]

    max_retries, wait_seconds = 3, 5
    for _ in range(max_retries):
        limiter.wait()
        try:
            completion = client.chat.completions.create(
                model=MODEL_NAME,
                messages=messages,
                stream=True,
                stream_options={"include_usage": True},
                temperature=0.1,
                top_p=0.95,
            )

            full_answer, full_reasoning = "", ""
            for chunk in completion:
                if not chunk.choices:
                    continue
                delta = chunk.choices[0].delta
                if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                    full_reasoning += delta.reasoning_content
                if hasattr(delta, 'content') and delta.content:
                    full_answer += delta.content

            json_data = parse_json_from_content(full_answer)
            if json_data is None:
                return {
                    "status": "PARSE_ERROR",
                    "critique": "Audit model output could not be parsed as JSON",
                    "_audit_thinking": full_reasoning,
                    "raw_response": full_answer,
                    "_parse_error": True
                }
            json_data["_audit_thinking"] = full_reasoning
            return json_data

        except Exception as e:
            print(f"  Warning: {e}")
            time.sleep(wait_seconds)
            wait_seconds *= 2

    return {"status": "TIMEOUT_ERROR", "critique": "Max retries exceeded", "_timeout_error": True}


def safe_append_row(path: str, fieldnames: list, row: dict):
    """Thread-safe CSV row append."""
    with file_lock:
        file_exists = os.path.isfile(path) and os.path.getsize(path) > 0
        with open(path, "a" if file_exists else "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            if not file_exists:
                writer.writeheader()
            writer.writerow(row)


FIELDNAMES = [
    ID_COLUMN, "original_vessel_count", "original_involved_vessels",
    "audit_status", "audit_critique", "audit_thinking", "original_reason"
]


def process_single_audit(row: pd.Series, limiter: ThreadSafeRateLimiter) -> bool:
    """Process one audit task."""
    case_id = str(row.get(ID_COLUMN, "")).strip()
    reasoning_content = str(row.get(REASONING_COLUMN, "")).strip()
    original_vessel_count = row.get("disease_vessel_count", "")
    original_involved_vessels = row.get("involved_vessels", "")
    original_reason = row.get("reason", "")

    print(f"Auditing: {case_id}...")

    if not reasoning_content or reasoning_content.lower() == "nan" or len(reasoning_content) < 20:
        print(f"  Skipped {case_id}: reasoning content too short")
        out_row = {
            ID_COLUMN: case_id,
            "original_vessel_count": original_vessel_count,
            "original_involved_vessels": original_involved_vessels,
            "audit_status": "SKIPPED",
            "audit_critique": "Reasoning content empty or too short",
            "audit_thinking": "",
            "original_reason": original_reason
        }
        safe_append_row(OUTPUT_CSV_FILE, FIELDNAMES, out_row)
        return True

    try:
        audit_result = call_audit_model(reasoning_content, limiter)
        out_row = {
            ID_COLUMN: case_id,
            "original_vessel_count": original_vessel_count,
            "original_involved_vessels": original_involved_vessels,
            "audit_status": audit_result.get("status", "UNKNOWN"),
            "audit_critique": audit_result.get("critique", ""),
            "audit_thinking": audit_result.get("_audit_thinking", ""),
            "original_reason": original_reason
        }
        safe_append_row(OUTPUT_CSV_FILE, FIELDNAMES, out_row)
        print(f"  Done: {case_id} | status={audit_result.get('status', 'UNKNOWN')}")
        return True

    except Exception as e:
        print(f"  Failed: {case_id}: {e}")
        out_row = {
            ID_COLUMN: case_id,
            "original_vessel_count": original_vessel_count,
            "original_involved_vessels": original_involved_vessels,
            "audit_status": "ERROR",
            "audit_critique": f"{type(e).__name__}: {e}",
            "audit_thinking": "",
            "original_reason": original_reason
        }
        safe_append_row(OUTPUT_CSV_FILE, FIELDNAMES, out_row)
        return False


# ========================== Main ==========================

def main():
    print("=" * 70)
    print("Diseased Vessel Counting — Reasoning Critique (Checker Model)")
    print("=" * 70)

    if not os.path.exists(INPUT_CSV_FILE):
        print(f"Input file not found: {INPUT_CSV_FILE}")
        return

    try:
        df = pd.read_csv(INPUT_CSV_FILE, encoding="utf-8-sig")
    except Exception as e:
        print(f"Failed to read input: {e}")
        return

    print(f"Total records: {len(df)}")

    if REASONING_COLUMN not in df.columns:
        print(f"Missing required column: {REASONING_COLUMN}")
        print(f"Available columns: {list(df.columns)}")
        return

    # Resume from checkpoint
    processed_ids = set()
    if os.path.exists(OUTPUT_CSV_FILE) and os.path.getsize(OUTPUT_CSV_FILE) > 0:
        try:
            df_done = pd.read_csv(OUTPUT_CSV_FILE, usecols=[ID_COLUMN], encoding="utf-8-sig")
            processed_ids = set(df_done[ID_COLUMN].astype(str).str.strip())
            print(f"[Checkpoint] Already processed: {len(processed_ids)}")
        except Exception as e:
            print(f"Checkpoint read error: {e}, starting fresh.")

    tasks = []
    for _, row in df.iterrows():
        case_id = str(row.get(ID_COLUMN, "")).strip()
        if not case_id or case_id == "nan":
            continue
        if case_id in processed_ids:
            continue
        tasks.append(row)

    print(f"Pending tasks: {len(tasks)}, workers: {MAX_WORKERS}")

    limiter = ThreadSafeRateLimiter(max_calls=CALLS_PER_MINUTE, window_seconds=WINDOW_SECONDS)
    success_count, fail_count = 0, 0

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(process_single_audit, row, limiter) for row in tasks]
        total = len(futures)
        for i, future in enumerate(as_completed(futures), 1):
            try:
                if future.result():
                    success_count += 1
                else:
                    fail_count += 1
            except Exception as e:
                fail_count += 1
                print(f"  Task exception: {e}")
            if i % 10 == 0:
                print(f"Progress: {i}/{total}")

    # Summary
    pass_count, fail_audit_count = 0, 0
    if os.path.exists(OUTPUT_CSV_FILE):
        try:
            df_result = pd.read_csv(OUTPUT_CSV_FILE, encoding="utf-8-sig")
            pass_count = (df_result["audit_status"] == "PASS").sum()
            fail_audit_count = (df_result["audit_status"] == "FAIL").sum()
        except Exception:
            pass

    print(f"\nCompleted: {total} total, {success_count} success, {fail_count} fail")
    print(f"Audit PASS: {pass_count}, FAIL: {fail_audit_count}")
    if (pass_count + fail_audit_count) > 0:
        print(f"Pass rate: {pass_count / (pass_count + fail_audit_count) * 100:.1f}%")
    print(f"Output: {OUTPUT_CSV_FILE}")


if __name__ == "__main__":
    main()
