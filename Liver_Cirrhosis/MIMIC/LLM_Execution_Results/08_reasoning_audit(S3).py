"""
Reasoning audit for liver cancer predictions (Strategy S3).

Deploys a checker model to verify the internal logic consistency
of AI-generated reasoning chains from the prediction step.
"""

import os
import json
import time
import csv
import re
import threading
import pandas as pd
from collections import deque
from concurrent.futures import ThreadPoolExecutor, as_completed
from openai import OpenAI

# =============================================================================
# Configuration
# =============================================================================

API_KEY    = os.getenv("DASHSCOPE_API_KEY", "sk-XXX")
BASE_URL   = "https://dashscope.aliyuncs.com/compatible-mode/v1"
MODEL_NAME = "qwen3-32b"

INPUT_CSV_FILE   = "path/to/liver_cancer_prediction_with_reasoning.csv"
OUTPUT_CHECK_FILE = "path/to/liver_reasoning_audit_results.csv"

MAX_WORKERS      = 30
CALLS_PER_MINUTE = 400
WINDOW_SECONDS   = 65

file_lock = threading.Lock()
client = OpenAI(api_key=API_KEY, base_url=BASE_URL)

# =============================================================================
# Audit Prompt
# =============================================================================

AUDIT_SYSTEM_PROMPT = """
You are a Medical Logic Auditor specializing in Liver Imaging (Non-contrast CT). Your task is to review a "Reasoning Chain" generated by an AI assistant and verify its **Internal Logic Consistency**.

## CONTEXT
- The reasoning is based on **Non-contrast (Plain) CT** only.
- Patient has Liver Cirrhosis.

## AUDIT CRITERIA (Check for these errors)

1. **Internal Contradiction**: Does the `Prediction` contradict the `CT Findings`?
   - *Error Example*: Findings say "simple cyst, water density, sharp margin", but Prediction is "High Risk/Malignancy".
   - *Error Example*: Findings say "No focal lesion", but Prediction says "Confirmed Cancer".

2. **Modality Hallucination**: Does the reasoning claim features impossible to see on Plain CT?
   - *Error*: Mentions of "Arterial phase enhancement", "Washout", "Hypervascularity", "Early enhancement", "Delayed washout" (Impossible without contrast agent).
   - *Note*: These contrast-related terms should NEVER appear in valid non-contrast CT reasoning.

3. **Logic Gap**: Is the conclusion supported by the text provided?
   - *Error Example*: Prediction is "confirmed_liver_cancer" but no mass or lesion is described.
   - *Error Example*: Prediction is "benign_lesion" but findings describe infiltrative margins and portal vein thrombus.

4. **Severity Mismatch**: Does the risk category match the findings severity?
   - *Error Example*: Findings describe "large infiltrative mass with PVTT" but category is "benign_lesion".
   - *Error Example*: Findings describe "simple cyst" but category is "high_risk".

## OUTPUT FORMAT
Please output JSON only, without markdown code blocks:
{
  "is_logically_sound": true or false,
  "flagged_issue": "None" or "Short description of the logic error",
  "error_type": "None" or "Contradiction / Hallucination / Unsupported Conclusion / Severity Mismatch",
  "audit_reasoning": "Brief explanation of why it passed or failed"
}
"""

# =============================================================================
# Utilities
# =============================================================================

class ThreadSafeRateLimiter:
    """Sliding-window rate limiter safe for concurrent use."""

    def __init__(self, max_calls: int, window_seconds: int):
        self.max_calls = max_calls
        self.window_seconds = window_seconds
        self.ts = deque()
        self.lock = threading.Lock()

    def wait(self):
        with self.lock:
            now = time.time()
            while self.ts and (now - self.ts[0]) >= self.window_seconds:
                self.ts.popleft()
            if len(self.ts) >= self.max_calls:
                sleep_s = self.window_seconds - (now - self.ts[0]) + 0.1
                if sleep_s > 0:
                    time.sleep(sleep_s)
                now = time.time()
                while self.ts and (now - self.ts[0]) >= self.window_seconds:
                    self.ts.popleft()
            self.ts.append(time.time())


def parse_json_response(content: str):
    """Extract JSON object from raw LLM output."""
    text = content.strip()
    if "```json" in text:
        text = re.sub(r"```json\s*", "", text)
        text = re.sub(r"```\s*$", "", text)
    elif text.startswith("```"):
        text = re.sub(r"^```[a-zA-Z0-9]*\n?", "", text)
        if text.endswith("```"):
            text = text[:-3].strip()
    text = text.strip()
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        start, end = text.find("{"), text.rfind("}")
        if start != -1 and end > start:
            try:
                return json.loads(text[start:end + 1])
            except Exception:
                pass
    return None


def call_audit_model(reasoning_detail, predicted_category, limiter):
    """Send reasoning chain to checker model for logic audit."""
    user_content = f"""
Please audit the following AI-generated reasoning chain for liver cancer prediction on non-contrast CT:

[Predicted Category]
{predicted_category}

[Reasoning Detail]
{reasoning_detail}

Check for internal contradictions, modality hallucinations (contrast-phase terminology in plain CT), and logic gaps.
Output your audit result in JSON format.
"""
    messages = [
        {"role": "system", "content": AUDIT_SYSTEM_PROMPT},
        {"role": "user",   "content": user_content},
    ]

    retries, wait = 3, 5
    for attempt in range(retries):
        limiter.wait()
        try:
            completion = client.chat.completions.create(
                model=MODEL_NAME, messages=messages,
                temperature=0.1, max_tokens=1024,
                extra_body={"enable_thinking": False},
            )
            content = completion.choices[0].message.content
            result = parse_json_response(content)
            if result:
                return result
            return {
                "is_logically_sound": "PARSE_ERROR",
                "flagged_issue": "Failed to parse JSON response",
                "error_type": "Parse Error",
                "audit_reasoning": content[:500],
            }
        except Exception as e:
            print(f"  API error (attempt {attempt + 1}/{retries}): {e}")
            time.sleep(wait); wait *= 2

    return {
        "is_logically_sound": "TIMEOUT",
        "flagged_issue": "API call failed after retries",
        "error_type": "Timeout", "audit_reasoning": "Retries exhausted",
    }


def safe_append_row(path, fieldnames, row):
    """Thread-safe CSV append."""
    with file_lock:
        exists = os.path.isfile(path) and os.path.getsize(path) > 0
        with open(path, "a" if exists else "w", newline="", encoding="utf-8-sig") as f:
            w = csv.DictWriter(f, fieldnames=fieldnames)
            if not exists:
                w.writeheader()
            w.writerow(row)


def process_single_task(row, id_column, limiter, fieldnames):
    """Audit one prediction record."""
    case_id    = str(row.get(id_column, "")).strip()
    reasoning  = str(row.get("reasoning_detail", "")).strip()
    category   = str(row.get("predicted_category", "")).strip()

    try:
        result = call_audit_model(reasoning, category, limiter)
        is_sound = result.get("is_logically_sound", "Unknown")

        out_row = {
            id_column: case_id,
            "predicted_category": category,
            "reasoning_detail_preview": reasoning[:200] + "..." if len(reasoning) > 200 else reasoning,
            "is_logically_sound": is_sound,
            "flagged_issue": result.get("flagged_issue", ""),
            "error_type": result.get("error_type", ""),
            "audit_reasoning": result.get("audit_reasoning", ""),
        }
        safe_append_row(OUTPUT_CHECK_FILE, fieldnames, out_row)

        if is_sound is True or str(is_sound).lower() == "true":
            status = "success"
            print(f"  {case_id} | {category} | PASSED")
        elif is_sound is False or str(is_sound).lower() == "false":
            status = "flagged"
            print(f"  {case_id} | {category} | FLAGGED: {result.get('error_type', 'Unknown')}")
        else:
            status = "error"
            print(f"  {case_id} | {category} | ERROR: {is_sound}")

        return {"status": status, "case_id": case_id}

    except Exception as e:
        print(f"  FAILED {case_id}: {e}")
        safe_append_row(OUTPUT_CHECK_FILE, fieldnames, {
            id_column: case_id, "predicted_category": category,
            "reasoning_detail_preview": reasoning[:200] + "..." if len(reasoning) > 200 else reasoning,
            "is_logically_sound": "ERROR",
            "flagged_issue": f"Processing error: {e}",
            "error_type": "Processing Error",
            "audit_reasoning": f"ERROR: {type(e).__name__} - {e}",
        })
        return {"status": "error", "case_id": case_id}


# =============================================================================
# Main
# =============================================================================

def main():
    print("=" * 70)
    print("Liver Reasoning Audit (Strategy S3 — Logic Critique)")
    print(f"Model: {MODEL_NAME}  |  Input: {INPUT_CSV_FILE}")
    print(f"Workers: {MAX_WORKERS} | RPM: {CALLS_PER_MINUTE}")
    print("=" * 70)

    if not os.path.exists(INPUT_CSV_FILE):
        print(f"Input file not found: {INPUT_CSV_FILE}"); return

    for enc in ["utf-8-sig", "utf-8", "gbk"]:
        try:
            df = pd.read_csv(INPUT_CSV_FILE, encoding=enc); break
        except Exception:
            continue
    else:
        print("Failed to read input file"); return

    print(f"Loaded {len(df)} prediction records")

    # Auto-detect ID column
    id_column = next((c for c in ["hadm_id", "case_id", "id"] if c in df.columns),
                     df.columns[0])

    fieldnames = [id_column, "predicted_category", "reasoning_detail_preview",
                  "is_logically_sound", "flagged_issue", "error_type", "audit_reasoning"]

    # Checkpoint resume
    processed_ids = set()
    if os.path.exists(OUTPUT_CHECK_FILE) and os.path.getsize(OUTPUT_CHECK_FILE) > 0:
        try:
            done = pd.read_csv(OUTPUT_CHECK_FILE, usecols=[id_column], encoding="utf-8-sig")
            processed_ids = set(done[id_column].astype(str).str.strip())
            print(f"Resuming — {len(processed_ids)} already audited")
        except Exception as e:
            print(f"Checkpoint error: {e}")

    tasks = [
        row for _, row in df.iterrows()
        if str(row.get(id_column, "")).strip() not in processed_ids
        and str(row.get("reasoning_detail", "")).strip() not in ("", "nan")
        and len(str(row.get("reasoning_detail", "")).strip()) >= 20
        and str(row.get("predicted_category", "")) not in ("ERROR", "TIMEOUT_ERROR", "PARSE_ERROR")
    ]
    print(f"Pending: {len(tasks)}")

    if not tasks:
        print("Nothing to process."); return

    limiter = ThreadSafeRateLimiter(CALLS_PER_MINUTE, WINDOW_SECONDS)
    ok, flagged, err = 0, 0, 0
    t0 = time.time()

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as pool:
        futures = [pool.submit(process_single_task, r, id_column, limiter, fieldnames) for r in tasks]
        for i, fut in enumerate(as_completed(futures), 1):
            try:
                s = fut.result()["status"]
                ok += (s == "success"); flagged += (s == "flagged"); err += (s == "error")
            except Exception as e:
                err += 1; print(f"  Exception: {e}")
            if i % 10 == 0 or i == len(futures):
                rate = i / (time.time() - t0) * 60
                print(f"Progress: {i}/{len(futures)} | pass={ok} flag={flagged} err={err} | {rate:.1f}/min")

    total = ok + flagged + err
    print(f"\nDone — {total} audited, {ok} passed ({ok / max(total, 1) * 100:.1f}%), "
          f"{flagged} flagged ({flagged / max(total, 1) * 100:.1f}%), {err} errors, "
          f"{(time.time() - t0) / 60:.1f} min")

    if os.path.exists(OUTPUT_CHECK_FILE):
        try:
            df_res = pd.read_csv(OUTPUT_CHECK_FILE, encoding="utf-8-sig")
            print("\nAudit result distribution:")
            print(df_res['is_logically_sound'].value_counts().to_string())
            flagged_df = df_res[df_res['is_logically_sound'] == False]
            if len(flagged_df) > 0:
                print("\nFlagged issue types:")
                print(flagged_df['error_type'].value_counts().to_string())
        except Exception as e:
            print(f"Stats error: {e}")


if __name__ == "__main__":
    main()
